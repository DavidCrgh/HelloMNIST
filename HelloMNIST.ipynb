{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#Hyperparameter definitions\n",
    "learning_rate = 1e-3\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "batch_size = 100\n",
    "epochs = 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 60000\n",
      "Test set size = 10000\n"
     ]
    }
   ],
   "source": [
    "#Import FashionMNIST training and test datasets\n",
    "training_data = datasets.FashionMNIST('data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = datasets.FashionMNIST('data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "print(f'Training set size = {len(training_data)}')\n",
    "print(f'Test set size = {len(test_data)}')\n",
    "\n",
    "#Create loaders for our datasets\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "MNIST_MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (h0): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (h1): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (h2): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Define our multilayer perceptron model by extending the nn.Module class\n",
    "class MNIST_MLP(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "      super(MNIST_MLP, self).__init__()\n",
    "\n",
    "      self.flatten = nn.Flatten()\n",
    "\n",
    "      self.h0 = nn.Sequential(\n",
    "          nn.Linear(input_dim, 512, bias=True),\n",
    "          nn.ReLU()\n",
    "      )\n",
    "      self.h1 = nn.Sequential(\n",
    "          nn.Linear(512, 512, bias=True),\n",
    "          nn.ReLU()\n",
    "      )\n",
    "      self.h2 = nn.Sequential(\n",
    "          nn.Linear(512, output_dim),\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      flat_x = self.flatten(x)\n",
    "      h0_outputs = self.h0(flat_x)\n",
    "      h1_outputs = self.h1(h0_outputs)\n",
    "      h2_outputs = self.h2(h1_outputs)\n",
    "\n",
    "      return h2_outputs\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "\n",
    "model = MNIST_MLP(input_dim, output_dim).to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#Define our loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(model.parameters()) # Use Adam with default hyperparams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#Define training and test loops\n",
    "def training_loop(dataloader, _model, loss_fn, optimizer):\n",
    "  size = len(dataloader.dataset)\n",
    "  for batch_number, (X, y) in enumerate(dataloader):\n",
    "    #Send batch tensors to device\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Run forward prop and calculate loss\n",
    "    pred = _model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    # Run backprop\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_number % 60 == 0: #Print loss every 60 batches (6000 samples)\n",
    "      loss, current_sample = loss.item(), batch_number * len(X)\n",
    "      print(f\"loss: {loss:>7f}   [{current_sample:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, _model, loss_fn):\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0,0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      #Send batch tensors to device\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      #Run inference, calculate loss and determine if prediction is correct\n",
    "      pred = _model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg. loss: {test_loss:>8f} \\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "-----------------------------------\n",
      "loss: 2.302933   [    0/60000]\n",
      "loss: 0.613497   [ 6000/60000]\n",
      "loss: 0.671353   [12000/60000]\n",
      "loss: 0.603223   [18000/60000]\n",
      "loss: 0.393405   [24000/60000]\n",
      "loss: 0.486514   [30000/60000]\n",
      "loss: 0.459886   [36000/60000]\n",
      "loss: 0.461749   [42000/60000]\n",
      "loss: 0.444568   [48000/60000]\n",
      "loss: 0.413948   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg. loss: 0.404855 \n",
      "\n",
      "Epoch: 2\n",
      "-----------------------------------\n",
      "loss: 0.346717   [    0/60000]\n",
      "loss: 0.319498   [ 6000/60000]\n",
      "loss: 0.344431   [12000/60000]\n",
      "loss: 0.583753   [18000/60000]\n",
      "loss: 0.462203   [24000/60000]\n",
      "loss: 0.300186   [30000/60000]\n",
      "loss: 0.459999   [36000/60000]\n",
      "loss: 0.408995   [42000/60000]\n",
      "loss: 0.287063   [48000/60000]\n",
      "loss: 0.273903   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg. loss: 0.383439 \n",
      "\n",
      "Epoch: 3\n",
      "-----------------------------------\n",
      "loss: 0.214381   [    0/60000]\n",
      "loss: 0.299496   [ 6000/60000]\n",
      "loss: 0.323769   [12000/60000]\n",
      "loss: 0.252697   [18000/60000]\n",
      "loss: 0.317236   [24000/60000]\n",
      "loss: 0.365719   [30000/60000]\n",
      "loss: 0.469672   [36000/60000]\n",
      "loss: 0.421743   [42000/60000]\n",
      "loss: 0.367146   [48000/60000]\n",
      "loss: 0.364035   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg. loss: 0.356290 \n",
      "\n",
      "Epoch: 4\n",
      "-----------------------------------\n",
      "loss: 0.283904   [    0/60000]\n",
      "loss: 0.254900   [ 6000/60000]\n",
      "loss: 0.305531   [12000/60000]\n",
      "loss: 0.284574   [18000/60000]\n",
      "loss: 0.293020   [24000/60000]\n",
      "loss: 0.314525   [30000/60000]\n",
      "loss: 0.250705   [36000/60000]\n",
      "loss: 0.270578   [42000/60000]\n",
      "loss: 0.291315   [48000/60000]\n",
      "loss: 0.336398   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg. loss: 0.342027 \n",
      "\n",
      "Epoch: 5\n",
      "-----------------------------------\n",
      "loss: 0.274904   [    0/60000]\n",
      "loss: 0.257994   [ 6000/60000]\n",
      "loss: 0.263146   [12000/60000]\n",
      "loss: 0.287672   [18000/60000]\n",
      "loss: 0.239864   [24000/60000]\n",
      "loss: 0.314998   [30000/60000]\n",
      "loss: 0.286455   [36000/60000]\n",
      "loss: 0.374753   [42000/60000]\n",
      "loss: 0.260764   [48000/60000]\n",
      "loss: 0.351707   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg. loss: 0.347310 \n",
      "\n",
      "Epoch: 6\n",
      "-----------------------------------\n",
      "loss: 0.161919   [    0/60000]\n",
      "loss: 0.300332   [ 6000/60000]\n",
      "loss: 0.289685   [12000/60000]\n",
      "loss: 0.243280   [18000/60000]\n",
      "loss: 0.264453   [24000/60000]\n",
      "loss: 0.295825   [30000/60000]\n",
      "loss: 0.139198   [36000/60000]\n",
      "loss: 0.211539   [42000/60000]\n",
      "loss: 0.236410   [48000/60000]\n",
      "loss: 0.274648   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg. loss: 0.360657 \n",
      "\n",
      "Epoch: 7\n",
      "-----------------------------------\n",
      "loss: 0.303147   [    0/60000]\n",
      "loss: 0.250189   [ 6000/60000]\n",
      "loss: 0.231297   [12000/60000]\n",
      "loss: 0.263051   [18000/60000]\n",
      "loss: 0.273312   [24000/60000]\n",
      "loss: 0.205682   [30000/60000]\n",
      "loss: 0.212397   [36000/60000]\n",
      "loss: 0.336186   [42000/60000]\n",
      "loss: 0.182338   [48000/60000]\n",
      "loss: 0.362673   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg. loss: 0.330196 \n",
      "\n",
      "Epoch: 8\n",
      "-----------------------------------\n",
      "loss: 0.259706   [    0/60000]\n",
      "loss: 0.329692   [ 6000/60000]\n",
      "loss: 0.228903   [12000/60000]\n",
      "loss: 0.189603   [18000/60000]\n",
      "loss: 0.217933   [24000/60000]\n",
      "loss: 0.244189   [30000/60000]\n",
      "loss: 0.216600   [36000/60000]\n",
      "loss: 0.203047   [42000/60000]\n",
      "loss: 0.390753   [48000/60000]\n",
      "loss: 0.318332   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg. loss: 0.317876 \n",
      "\n",
      "Epoch: 9\n",
      "-----------------------------------\n",
      "loss: 0.300526   [    0/60000]\n",
      "loss: 0.127291   [ 6000/60000]\n",
      "loss: 0.167650   [12000/60000]\n",
      "loss: 0.241540   [18000/60000]\n",
      "loss: 0.214912   [24000/60000]\n",
      "loss: 0.260928   [30000/60000]\n",
      "loss: 0.255709   [36000/60000]\n",
      "loss: 0.164934   [42000/60000]\n",
      "loss: 0.234222   [48000/60000]\n",
      "loss: 0.252735   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg. loss: 0.318123 \n",
      "\n",
      "Epoch: 10\n",
      "-----------------------------------\n",
      "loss: 0.234243   [    0/60000]\n",
      "loss: 0.231709   [ 6000/60000]\n",
      "loss: 0.180674   [12000/60000]\n",
      "loss: 0.184055   [18000/60000]\n",
      "loss: 0.255096   [24000/60000]\n",
      "loss: 0.128960   [30000/60000]\n",
      "loss: 0.138505   [36000/60000]\n",
      "loss: 0.223403   [42000/60000]\n",
      "loss: 0.275537   [48000/60000]\n",
      "loss: 0.224198   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg. loss: 0.330647 \n",
      "\n",
      "Epoch: 11\n",
      "-----------------------------------\n",
      "loss: 0.233426   [    0/60000]\n",
      "loss: 0.228335   [ 6000/60000]\n",
      "loss: 0.323563   [12000/60000]\n",
      "loss: 0.197360   [18000/60000]\n",
      "loss: 0.286285   [24000/60000]\n",
      "loss: 0.245492   [30000/60000]\n",
      "loss: 0.203842   [36000/60000]\n",
      "loss: 0.225440   [42000/60000]\n",
      "loss: 0.262865   [48000/60000]\n",
      "loss: 0.232927   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg. loss: 0.333737 \n",
      "\n",
      "Epoch: 12\n",
      "-----------------------------------\n",
      "loss: 0.260918   [    0/60000]\n",
      "loss: 0.176580   [ 6000/60000]\n",
      "loss: 0.206694   [12000/60000]\n",
      "loss: 0.125565   [18000/60000]\n",
      "loss: 0.202292   [24000/60000]\n",
      "loss: 0.203112   [30000/60000]\n",
      "loss: 0.214455   [36000/60000]\n",
      "loss: 0.215066   [42000/60000]\n",
      "loss: 0.131591   [48000/60000]\n",
      "loss: 0.240091   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg. loss: 0.314975 \n",
      "\n",
      "Epoch: 13\n",
      "-----------------------------------\n",
      "loss: 0.155600   [    0/60000]\n",
      "loss: 0.234766   [ 6000/60000]\n",
      "loss: 0.099458   [12000/60000]\n",
      "loss: 0.191464   [18000/60000]\n",
      "loss: 0.209669   [24000/60000]\n",
      "loss: 0.171867   [30000/60000]\n",
      "loss: 0.196431   [36000/60000]\n",
      "loss: 0.172280   [42000/60000]\n",
      "loss: 0.168723   [48000/60000]\n",
      "loss: 0.181109   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg. loss: 0.320249 \n",
      "\n",
      "Epoch: 14\n",
      "-----------------------------------\n",
      "loss: 0.251821   [    0/60000]\n",
      "loss: 0.202038   [ 6000/60000]\n",
      "loss: 0.202241   [12000/60000]\n",
      "loss: 0.260061   [18000/60000]\n",
      "loss: 0.110250   [24000/60000]\n",
      "loss: 0.277078   [30000/60000]\n",
      "loss: 0.138565   [36000/60000]\n",
      "loss: 0.218466   [42000/60000]\n",
      "loss: 0.130015   [48000/60000]\n",
      "loss: 0.169325   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg. loss: 0.326884 \n",
      "\n",
      "Epoch: 15\n",
      "-----------------------------------\n",
      "loss: 0.093055   [    0/60000]\n",
      "loss: 0.119519   [ 6000/60000]\n",
      "loss: 0.114736   [12000/60000]\n",
      "loss: 0.231843   [18000/60000]\n",
      "loss: 0.168706   [24000/60000]\n",
      "loss: 0.262845   [30000/60000]\n",
      "loss: 0.169298   [36000/60000]\n",
      "loss: 0.180257   [42000/60000]\n",
      "loss: 0.217919   [48000/60000]\n",
      "loss: 0.149519   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.0%, Avg. loss: 0.381189 \n",
      "\n",
      "Epoch: 16\n",
      "-----------------------------------\n",
      "loss: 0.174837   [    0/60000]\n",
      "loss: 0.174513   [ 6000/60000]\n",
      "loss: 0.247577   [12000/60000]\n",
      "loss: 0.158284   [18000/60000]\n",
      "loss: 0.131224   [24000/60000]\n",
      "loss: 0.148214   [30000/60000]\n",
      "loss: 0.177593   [36000/60000]\n",
      "loss: 0.177137   [42000/60000]\n",
      "loss: 0.175069   [48000/60000]\n",
      "loss: 0.223312   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg. loss: 0.326200 \n",
      "\n",
      "Epoch: 17\n",
      "-----------------------------------\n",
      "loss: 0.108546   [    0/60000]\n",
      "loss: 0.172347   [ 6000/60000]\n",
      "loss: 0.137851   [12000/60000]\n",
      "loss: 0.091692   [18000/60000]\n",
      "loss: 0.148216   [24000/60000]\n",
      "loss: 0.182963   [30000/60000]\n",
      "loss: 0.108248   [36000/60000]\n",
      "loss: 0.207491   [42000/60000]\n",
      "loss: 0.200518   [48000/60000]\n",
      "loss: 0.116089   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg. loss: 0.343598 \n",
      "\n",
      "Epoch: 18\n",
      "-----------------------------------\n",
      "loss: 0.254378   [    0/60000]\n",
      "loss: 0.148241   [ 6000/60000]\n",
      "loss: 0.073224   [12000/60000]\n",
      "loss: 0.155614   [18000/60000]\n",
      "loss: 0.141484   [24000/60000]\n",
      "loss: 0.111671   [30000/60000]\n",
      "loss: 0.251011   [36000/60000]\n",
      "loss: 0.108203   [42000/60000]\n",
      "loss: 0.142419   [48000/60000]\n",
      "loss: 0.154573   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg. loss: 0.339474 \n",
      "\n",
      "Epoch: 19\n",
      "-----------------------------------\n",
      "loss: 0.109403   [    0/60000]\n",
      "loss: 0.149650   [ 6000/60000]\n",
      "loss: 0.148060   [12000/60000]\n",
      "loss: 0.155048   [18000/60000]\n",
      "loss: 0.101639   [24000/60000]\n",
      "loss: 0.168177   [30000/60000]\n",
      "loss: 0.189043   [36000/60000]\n",
      "loss: 0.084586   [42000/60000]\n",
      "loss: 0.179333   [48000/60000]\n",
      "loss: 0.109372   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg. loss: 0.343854 \n",
      "\n",
      "Epoch: 20\n",
      "-----------------------------------\n",
      "loss: 0.239483   [    0/60000]\n",
      "loss: 0.182489   [ 6000/60000]\n",
      "loss: 0.075245   [12000/60000]\n",
      "loss: 0.193287   [18000/60000]\n",
      "loss: 0.131967   [24000/60000]\n",
      "loss: 0.174506   [30000/60000]\n",
      "loss: 0.111063   [36000/60000]\n",
      "loss: 0.218735   [42000/60000]\n",
      "loss: 0.189000   [48000/60000]\n",
      "loss: 0.114579   [54000/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg. loss: 0.350975 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run training and test loops\n",
    "for i in range(epochs):\n",
    "  print(f'Epoch: {i+1}\\n-----------------------------------')\n",
    "  training_loop(training_loader, model, loss_fn, optimizer)\n",
    "  test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_filename = 'helloMNIST_model.pth'\n",
    "\n",
    "torch.save(model.state_dict(), model_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new network, load the model into it and run an additional 20 epochs\n",
    "new_model = MNIST_MLP(input_dim, output_dim)\n",
    "new_model.load_state_dict(torch.load(model_filename))\n",
    "new_model.to(device)\n",
    "\n",
    "# Create a new optimizer for new model's parameters\n",
    "new_optimizer = optim.Adam(new_model.parameters())\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f'Epoch: {i+1}\\n-----------------------------------')\n",
    "  training_loop(training_loader, new_model, loss_fn, new_optimizer)\n",
    "  test_loop(test_loader, new_model, loss_fn)\n",
    "\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}